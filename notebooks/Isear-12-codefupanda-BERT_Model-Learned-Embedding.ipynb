{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Softmax, GRU, Input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# utils\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_relative_path = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    print(\"Colab babe\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "    os.system('cp \"/content/gdrive/My Drive/Colab Notebooks/.kaggle/kaggle.json\" /root/.kaggle/kaggle.json')\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/codefupanda/customer_interaction_summary.git\"])\n",
    "    os.system('cd customer_interaction_summary && make requirements && make data > logs.logs')\n",
    "    parent_relative_path = \"./customer_interaction_summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "isear = pd.read_csv(parent_relative_path + '/data/raw/isear.csv', sep='|', error_bad_lines=False, usecols=['Field1', 'SIT', 'EMOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(isear.EMOT.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=128\n",
    "input_word_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"input_mask\")\n",
    "segment_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,name=\"segment_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer=FullTokenizer(vocab_file,do_lower_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_input(sentence,MAX_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    "\n",
    "  return ids,masks,segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_array(sentences):\n",
    "\n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    "\n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    "\n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "out = tf.keras.layers.Dense(number_of_classes+1, activation=\"softmax\", name=\"dense_output\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'create_input_array' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-57ad418b6642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SIT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CVxTz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EMOT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_input_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model.fit(x_train, y_train,epochs=5,batch_size=32,validation_data=(x_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_input_array' is not defined"
     ]
    }
   ],
   "source": [
    "train_sentences = isear['SIT'].fillna(\"CVxTz\").values\n",
    "train_y = isear['EMOT']\n",
    "inputs=create_input_array(train_sentences)\n",
    "model.fit(inputs,train_y,epochs=5,batch_size=32,validation_split=0.2,shuffle=True)\n",
    "#model.fit(x_train, y_train,epochs=5,batch_size=32,validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = isear['SIT'].fillna(\"CVxTz\").values\n",
    "\n",
    "test_inputs=create_input_array(test_sentences[110:150])\n",
    "\n",
    "print(model.predict(test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}